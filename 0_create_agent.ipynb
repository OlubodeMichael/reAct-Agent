{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b7114f7-abee-4b4f-a9bf-fbd7ac553357",
   "metadata": {},
   "source": [
    "**Load environmental variables**: See \"Getting setup\" in the first modele.  `dotenv` [docs](https://pypi.org/project/python-dotenv/) will look for `../.env`. If it finds it, it will load environmental variables from there, overriding any variables in the current shell. If it is not found, variables currently in the shell are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb48f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.14.0)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"/Users/lacatel/Desktop/Ai Agent/reAct-langChain/.venv/bin/python\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.join(\"..\", \".env\"), override=True)\n",
    "\n",
    "# automatically reload all modules before executing new code. The captures changes in local packages.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09dd8911",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Create React Agent  - Prebuilt\n",
    "\n",
    "<img src=\"./assets/agent_header.png\" width=\"800\" style=\"display:block; margin-left:0;\">\n",
    "\n",
    "In this course, you're going to build a [Deep Agent](https://blog.langchain.com/deep-agents/). We'll build this on top of LangGraph's 'pre-built' agent abstraction, which simplifies the code significantly. In this lesson, you'll learn about the pre-built ReAct agent. Here's what you will learn:\n",
    "- What is a ReAct Agent\n",
    "- The capabilities of our implementation and where to find out more.\n",
    "    - Build an agent with tools\n",
    "    - The graph, state and messages\n",
    "    - Access and modify state with tools\n",
    "    - <span style=\"font-size:0.8em;\">ü™ù</span> hooks! and structured responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96d31dc-3ffb-4afb-85ef-5e10f9a85f24",
   "metadata": {},
   "source": [
    "### What is a ReAct agent \n",
    "\n",
    "\n",
    "<img src=\"./assets/agent.png\"\n",
    "     style=\"float:left; max-width:300px; height:auto; margin:0 1rem 0.5rem 0;\">\n",
    "<div style=\"max-width: 1250px;\">\n",
    "    \n",
    "You will be using LangGraph's open-source `create_react_agent` ([see here](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent)) abstraction. A **ReAct agent** is an AI agent that uses the \"Reasoning and Acting\" (ReAct) framework to combine chain-of-thought (CoT) reasoning with external tool use. It was made popular by the paper [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629). \n",
    "\n",
    "This agent consists of three components: a large language model (LLM), a set of tools it can use, and a prompt that provides instructions.\n",
    "\n",
    "The LLM operates in a loop. In each iteration, it examines its context, which includes a list of available tools; It decides if it needs to call a tool. It selects a tool to invoke, forms the tool call. This is sent to a tool node for execution. The tool node executes the tool(s), and sends the results (observations) back to the LLM. The LLM receives the observations(s) and uses that observation to inform the next action. The loop continues until a stopping condition is met ‚Äî typically when the agent decides it no longer needs to call more tools.\n",
    "</div>\n",
    "\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    ">  Note: The `create_react_agent` is moving in the soon-to-be-released V1! It will be in LangChain and the name changed to simple `create_agent`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "038dca59",
   "metadata": {},
   "source": [
    "### Key capabilities of LangGraph pre-built `create_react_agent`\n",
    "\n",
    "Here are some of the features that come with the `create_react_agent` abstraction. We won't make use of them all in this course, but it does motivate the use of `create_react_agent` to build our course(learn more [here](https://langchain-ai.github.io/langgraph/agents/overview/#what-is-an-agent)):\n",
    "\n",
    "- [Memory integration](../how-tos/memory/add-memory.md): Native support for _short-term_ (session-based) and _long-term_ (persistent across sessions) memory, enabling stateful behaviors in chatbots and assistants.\n",
    "- [Human-in-the-loop control](../concepts/human_in_the_loop.md): Execution can pause _indefinitely_ to await human feedback‚Äîunlike websocket-based solutions limited to real-time interaction. This enables asynchronous approval, correction, or intervention at any point in the workflow.\n",
    "- [Streaming support](../how-tos/streaming.md): Real-time streaming of agent state, model tokens, tool outputs, or combined streams.\n",
    "- [Deployment tooling](../tutorials/langgraph-platform/local-server.md): Includes infrastructure-free deployment tools. [**LangGraph Platform**](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/) supports testing, debugging, and deployment.\n",
    "  - [Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/): A visual IDE for inspecting and debugging workflows.\n",
    "  - [LangSmith](https://smith.langchain.com/): A tracing and evaluation tool.\n",
    "  - Supports multiple [deployment options](https://langchain-ai.github.io/langgraph/concepts/deployment_options.md) for production.\n",
    "\n",
    "`create-react-agent` is quite sophisticated, accepting many input formats, allowing lots of customization. It can be a simple agent-tool loop, or can add customized with pre/post-hooks and/or structured outputs.\n",
    "\n",
    "<div style=\"display:flex; justify-content:center; align-items:flex-end; gap:40px;\">\n",
    "\n",
    "  <figure style=\"margin:0; text-align:center; width:250px;\">\n",
    "    <img src=\"./assets/simple_agent.png\" style=\"max-height:350px; width:auto; display:block; margin:0 auto;\">  \n",
    "    <figcaption style=\"font-weight:bold; font-size:1.1em; margin-top:8px;\">simple agent</figcaption>\n",
    "  </figure>\n",
    "\n",
    "  <figure style=\"margin:0; text-align:center; width:250px;\">\n",
    "    <img src=\"./assets/complex_agent.png\" style=\"max-height:350px; width:auto; display:block; margin:0 auto;\">  \n",
    "    <figcaption style=\"font-weight:bold; font-size:1.1em; margin-top:8px;\">complex agent</figcaption>\n",
    "  </figure>\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "In this course, you will use the simple format - though you will add sub-agents <span style=\"font-size:20px;\">ü§ñ ü§ñ ü§ñ</span> later in the course!   \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee33aed8-3c03-4fe3-b2ba-ae448ad86111",
   "metadata": {},
   "source": [
    "#### Build an agent with tools\n",
    "\n",
    "Let's start by creating an agent with a simple calculator tool to get started. Once you see how things are put together, we'll go over more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dbbaf1-a3d2-4094-80c6-55dbb1632f21",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.14.0)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"/Users/lacatel/Desktop/Ai Agent/reAct-langChain/.venv/bin/python\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, List, Literal, Union\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "@tool\n",
    "def calculator(\n",
    "    operation: Literal[\"add\",\"subtract\",\"multiply\",\"divide\"],\n",
    "    a: Union[int, float],\n",
    "    b: Union[int, float],\n",
    ") -> Union[int, float]:\n",
    "    \"\"\"Define a two-input calculator tool.\n",
    "\n",
    "    Arg:\n",
    "        operation (str): The operation to perform ('add', 'subtract', 'multiply', 'divide').\n",
    "        a (float or int): The first number.\n",
    "        b (float or int): The second number.\n",
    "        \n",
    "    Returns:\n",
    "        result (float or int): the result of the operation\n",
    "    Example\n",
    "        Divide: result   = a / b\n",
    "        Subtract: result = a - b\n",
    "    \"\"\"\n",
    "    if operation == 'divide' and b == 0:\n",
    "        return {\"error\": \"Division by zero is not allowed.\"}\n",
    "\n",
    "    # Perform calculation\n",
    "    if operation == 'add':\n",
    "        result = a + b\n",
    "    elif operation == 'subtract':\n",
    "        result = a - b\n",
    "    elif operation == 'multiply':\n",
    "        result = a * b\n",
    "    elif operation == 'divide':\n",
    "        result = a / b\n",
    "    else: \n",
    "        result = \"unknown operation\"\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c0fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from utils import format_messages\n",
    "\n",
    "# Create agent using create_react_agent directly\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful arithmetic assistant who is an expert at using a calculator.\"\n",
    "\n",
    "model = init_chat_model(model=\"openai:gpt-4.1-mini\", temperature=0.0)\n",
    "tools = [calculator]\n",
    "\n",
    "# Create agent\n",
    "agent = create_react_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    prompt=SYSTEM_PROMPT,\n",
    "    #state_schema=AgentState,  # default\n",
    ").with_config({\"recursion_limit\": 20})  #recursion_limit limits the number of steps the agent will run\n",
    "\n",
    "# Show the agent\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2aa0ee-0e6b-49ac-9085-39f5f50969ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_react_agent returns a compiled graph\n",
    "type(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf73e64-eb00-4d13-89cb-5857e4861f34",
   "metadata": {},
   "source": [
    "###  The graph, state and messages\n",
    "You'll run the agent in a moment, but let's dig into the graph a little bit. \n",
    "You can examine the code that implements `create_react_agent` [here](https://github.com/langchain-ai/langgraph/blob/c37c9cbab3287f0988fabe2b853569a23960e3db/libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py) if you would like to see the details.  If you would like to try building a simple version of this yourself, you can check out [Foundation: Introduction to LangGraph, Module 1, Lesson 6, Agent](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239232-lesson-6-agent).\n",
    "\n",
    "**Defining the Agent**: When you define an agent as you did above, you provide: the model, one or more tools, a \"system\" prompt, and state schema which defaults to <a href=\"https://github.com/langchain-ai/langgraph/blob/e365b2b8bd695e03d758b19ff109152b2e342a87/libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py#L62-L69\">\n",
    "  <code style=\"color:#0366d6;\">AgentState</code>\n",
    "</a> which is primarily a list of messages. ([Call details here.](https://langchain-ai.github.io/langgraph/reference/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent))\n",
    "Under the hood, this is defining and compiling the LangGraph graph shown above. An important detail is that the tools node is another pre-built item, a `ToolNode`, described [here](https://github.com/langchain-ai/langgraph/blob/e365b2b8bd695e03d758b19ff109152b2e342a87/libs/prebuilt/langgraph/prebuilt/tool_node.py#L239-L293). A tool node will run all the tools identified in the message from the LLM and return the results.\n",
    "\n",
    "**Invoking the Agent:** \n",
    "Let's call the agent and see what we get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2648f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "result1 = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is 3.1 * 4.2?\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "format_messages(result1[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b437e65-8b4f-48ec-9ab5-b988fe6b536d",
   "metadata": {},
   "source": [
    "**Invoking the Agent:** \n",
    "A sequence diagram is a great way to look at what happens when the model is invoked.\n",
    "\n",
    "<div style=\"display:none\">\n",
    "    the mermaid code is saved here for future\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant U as User\n",
    "    participant A as LLM\n",
    "    participant T as Tools\n",
    "    Note over A: System message<br/>\"You are a helpful assistant...\"\n",
    "    U->>A: Initial input,<br/>\"What is 3.1 * 4.2?\"\n",
    "    loop while tool_calls present\n",
    "        A->>T: AIMessage(id=\"call_123\", tool_calls=[...])\n",
    "        T-->>A: ToolMessage(tool_call_id=\"call_123\", content=\"xx\")\n",
    "    end\n",
    "    A->>U: Return final state\n",
    "```\n",
    "</div> \n",
    "\n",
    "<img src=\"./assets/agent_sequence_diagram.png\"\n",
    "     style=\"float:left; max-width:500px; height:auto; margin:0 1rem 0.5rem 0;\">\n",
    "<div style=\"max-width: 1100px;\">\n",
    "In our example, the user input is \"What is 3.1 * 4.2?\". This, combined with the system prompt and tool descriptions, is sent to the LLM.  \n",
    "<br/>\n",
    "<p style=\"margin-bottom:0; margin-top:5px;\">The LLM decides that the calculator tool should be called. <br/> It adds an `AIMessage` to `messages`:</p>\n",
    "<pre style=\"font-size:0.85em; margin-top:5px; margin-bottom:0;\">\n",
    "<code class=\"language-python\">AIMessage(\n",
    "    content=\"\",\n",
    "    tool_calls=[{\"id\": \"call_123\",\n",
    "                 \"name\": \"calculator\",\n",
    "                 \"args\": {\"a\": 3.1, \"b\": 4.2, \"operation\": \"multiply\"}}])</code></pre>   \n",
    "</code></pre>\n",
    "<br/>\n",
    "<p style=\"margin-bottom:0;\">The tool node receives the AIMessage and processes all the tool calls. It tracks the tool_call_ids. It responds with a ToolMessage in `messages`: </p>\n",
    "<pre style=\"font-size:0.85em; margin-top:5px; margin-bottom:10px;\">\n",
    "<code class=\"language-python\">ToolMessage(\n",
    "    content=\"13.02\",         # The result of the tool execution.\n",
    "    tool_call_id=\"call_123\")  # Matches the id from the AIMessage.tool_calls\n",
    "</code></pre>\n",
    "The LLM examines the response in `messages`, decides it is done, and forms an `AIMessage` to the user.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"clear:both;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784297e4-a617-41f6-a300-9ff06598c023",
   "metadata": {},
   "source": [
    "Let's look at the [trace in LangSmith](https://smith.langchain.com/public/3d2062e3-8713-4510-a797-801abe44d1f7/r). Here are some things to notice:\n",
    "- In the metadata of the call to the LLM, you will see the 'calculator' tool description.\n",
    "- The response from the model is a tool call with the arguments\n",
    "- In the final call to the LLM, notice the matching tool_call_id's provided by the tool node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0378644e-7630-40f7-946f-00f4e5d3a912",
   "metadata": {},
   "source": [
    "#### Try your own\n",
    "Take a moment and try this on your own. Run a query, check it in LangSmith and see if it matches your expectation. Try expanding the calculator - maybe add a square function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb3e30b-dec9-43a4-a340-6aee9370bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own \n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is ...?\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "format_messages(result[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146bef29-b1f2-45e1-883b-5fc51f6d54df",
   "metadata": {},
   "source": [
    "### Access and modify state within tools\n",
    "#### State\n",
    "One of the nice features of LangGraph is state. The graph has a typed data structure that is available to each node for the duration of the graph and can be persisted in long-term storage. You can use this to store information to share between nodes, to debug the graph, and to reset a long-running graph to an earlier time.\n",
    "\n",
    "When you define state for a graph, you define the data types and a 'reducer' function. The reducer describes how information is added to that element. This is especially useful when a task is mapped to multiple nodes, which are executed in parallel and update state simultaneously.\n",
    "\n",
    "In this example, the default `AgentState` was used. This is defined in [langgraph.prebuilt.chat_agent_executor](https://github.com/langchain-ai/langgraph/blob/e365b2b8bd695e03d758b19ff109152b2e342a87/libs/prebuilt/langgraph/prebuilt/chat_agent_executor.py).   \n",
    "\n",
    "```python\n",
    "    class AgentState(TypedDict):\n",
    "        \"\"\"The state of the agent.\"\"\"\n",
    "        messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "        remaining_steps: NotRequired[RemainingSteps]\n",
    "```\n",
    "        \n",
    "- `messages` are a list of `BaseMessage`, defined in [langchain_core](https://github.com/langchain-ai/langchain/blob/088095b663993b1e53cf616e1ca487d1739b0d71/libs/core/langchain_core/messages/base.py), which contains the messages to and from the LLM.\n",
    "    - typing.Annotated allows you to attach arbitrary metadata to a type hint. Syntax: Annotated[Type, metadata1, metadata2, ...] \n",
    "- The `add_messages` reducer will append new messages to the end of the message list.  \n",
    "- `remaining_steps` tracks the steps in a graph. You will see this initialized as the `recursion_limit`, but is tracked by the graph and not visibile to the user.  \n",
    "Let's take a look at this quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35787543-06dc-43cb-8f18-2abbf7bf3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "from langchain_core.messages import messages_to_dict\n",
    "\n",
    "JSON({\"messages\": messages_to_dict(result1[\"messages\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4720982e-16c2-4b90-84d6-94aba6a13e04",
   "metadata": {},
   "source": [
    "#### Custom State\n",
    "Let's extend our calculator to keep a list of all of the operations that have been performed. This will require adding a list to state, and a reducer function to add the state to the list. This will safely handle the case where the list or operation is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea8d331-3e9c-4195-99a1-fa5cf6659c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "\n",
    "def reduce_list(left: list | None, right: list | None) -> list:\n",
    "    \"\"\"Safely combine two lists, handling cases where either or both inputs might be None.\n",
    "\n",
    "    Args:\n",
    "        left (list | None): The first list to combine, or None.\n",
    "        right (list | None): The second list to combine, or None.\n",
    "\n",
    "    Returns:\n",
    "        list: A new list containing all elements from both input lists.\n",
    "               If an input is None, it's treated as an empty list.\n",
    "    \"\"\"\n",
    "    if not left:\n",
    "        left = []\n",
    "    if not right:\n",
    "        right = []\n",
    "    return left + right\n",
    "\n",
    "class CalcState(AgentState):\n",
    "    \"\"\"Graph State.\"\"\"\n",
    "    ops: Annotated[List[str], reduce_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd4660-83b9-4a23-9108-a03cbd26877a",
   "metadata": {},
   "source": [
    "#### Accessing State \n",
    "Now, we can extend our calculator to include the update. This highlights an issue! Now state is an argument to our calculator tool. \n",
    "<img src=\"./assets/state_arg_diagram.png\" width=\"800\" style=\"display:block; margin-left:0;\">\n",
    "In the diagram, it's clear that, while the LLM is tasked with generating the tool call, it cannot form the `state` argument as it does not have that in its context!  \n",
    "The solution is to **inject the state** after the LLM.\n",
    "<img src=\"./assets/inject_state_diagram.png\" width=\"1000\" style=\"display:block; margin-left:0;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01aea69-1704-48ff-b33d-7484e35a691b",
   "metadata": {},
   "source": [
    "<div style=\"margin:0; padding:0\">\n",
    "  <p style=\"margin:0;\">This is accomplished with the <code>InjectedState</code> annotation as shown below.</p>\n",
    "  <pre style=\"font-size:0.90em; margin:0; padding-top:0;\">\n",
    "<code class=\"language-python\">    @tool\n",
    "    def calculator_wstate(\n",
    "        operation: Literal[\"add\",\"subtract\",\"multiply\",\"divide\"],\n",
    "        a: Union[int, float],\n",
    "        b: Union[int, float],\n",
    "        <span style=\"background:#fff3a3; padding:0 2px;\">state: Annotated[CalcState, InjectedState],</span>  # ‚Üê not sent to LLM\n",
    "        <span style=\"background:#fff3a3; padding:0 2px;\">tool_call_id: Annotated[str, InjectedToolCallId],</span>  # ‚Üê not sent to LLM\n",
    "    ) -> Union[int, float]:\n",
    "</code></pre>\n",
    "<p style=\"margin:0; padding-top:4px;\">\n",
    "    This strips <code>state</code> from the description provided to the LLM, and injects it when calling the tool in <code>ToolNode</code>. the <code>tool_call_id</code> is also included. This is explained in the next section.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcaea07-fa1b-4b1a-bd1f-c7e77a6fe451",
   "metadata": {},
   "source": [
    "#### Updating State\n",
    "You may recall that tools typically return their observations to the LLM in a `ToolMessage` that is included in `messages` field in state. To update additional members of state, we would like to extend this update.  This is done using `Command` as in the return below. \n",
    "```python\n",
    "    return Command(\n",
    "        update={\n",
    "            \"ops\": ops,\n",
    "            \"messages\": [\n",
    "                ToolMessage(f\"{result}\", tool_call_id=tool_call_id)\n",
    "            ]})\n",
    "```\n",
    "Note that to create a `ToolMessage` we needed the `tool_call_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ed717-9390-437d-991c-1df0d9a337e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculator_wstate(\n",
    "    operation: Literal[\"add\",\"subtract\",\"multiply\",\"divide\"],\n",
    "    a: Union[int, float],\n",
    "    b: Union[int, float],\n",
    "    state: Annotated[CalcState, InjectedState],   # not sent to LLM\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId] # not sent to LLM\n",
    ") -> Union[int, float]:\n",
    "    \"\"\"Define a two-input calculator tool.\n",
    "\n",
    "    Arg:\n",
    "        operation (str): The operation to perform ('add', 'subtract', 'multiply', 'divide').\n",
    "        a (float or int): The first number.\n",
    "        b (float or int): The second number.\n",
    "        \n",
    "    Returns:\n",
    "        result (float or int): the result of the operation\n",
    "    Example\n",
    "        Divide: result   = a / b\n",
    "        Subtract: result = a - b\n",
    "    \"\"\"\n",
    "    if operation == 'divide' and b == 0:\n",
    "        return {\"error\": \"Division by zero is not allowed.\"}\n",
    "\n",
    "    # Perform calculation\n",
    "    if operation == 'add':\n",
    "        result = a + b\n",
    "    elif operation == 'subtract':\n",
    "        result = a - b\n",
    "    elif operation == 'multiply':\n",
    "        result = a * b\n",
    "    elif operation == 'divide':\n",
    "        result = a / b\n",
    "    else: \n",
    "        result = \"unknown operation\"\n",
    "    ops = [f\"({operation}, {a}, {b}),\" ]\n",
    "    return Command(\n",
    "        update={\n",
    "            \"ops\": ops,\n",
    "            \"messages\": [\n",
    "                ToolMessage(f\"{result}\", tool_call_id=tool_call_id)\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5163b6-673a-4e01-a226-34c9924f837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are a helpful arithmetic assistant who is an expert at using a calculator.\"\n",
    "\n",
    "model = init_chat_model(model=\"openai:gpt-4o-mini\", temperature=0.0)\n",
    "tools = [calculator_wstate]  # new tool\n",
    "\n",
    "# Create agent\n",
    "agent = create_react_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    prompt=SYSTEM_PROMPT,\n",
    "    state_schema=CalcState,  # now defining state scheme\n",
    ").with_config({\"recursion_limit\": 20})  #recursion_limit limits the number of steps the agent will run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f666a44a-ada6-4d64-9dfd-39dfa04533b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "result2 = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is 3.1 * 4.2?\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "format_messages(result2[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55b1e3-ccba-4899-bbd3-6ab5fb729716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the ops field is now in the response\n",
    "JSON(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f86a4-53b4-4814-9ee5-cb8919a1c9c4",
   "metadata": {},
   "source": [
    "Let's try one more example. Notice the dual tool call in this example. The tool node will execute these in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15902d-064c-49fe-8266-c9cc5e895344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "result3 = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is 3.1 * 4.2 + 5.5 * 6.5?\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "format_messages(result3[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddba925-7132-4972-94d2-33e21c15ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48168b8-54bb-4b72-a494-dccbdbf2c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage - create your own\n",
    "result4 = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Create an example of your own?\",\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "format_messages(result4[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93063f-9625-4667-b9d8-407b1103d4f9",
   "metadata": {},
   "source": [
    "## <span style=\"font-size:0.8em;\">ü™ù</span> Hooks and structured responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234cce2b-7124-4933-8c34-9b5a409b4683",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td style=\"width:300px;\">\n",
    "  <img src=\"./assets/complex_agent.png\" style=\"max-width:100%; height:auto;\">\n",
    "</td>\n",
    "<td>\n",
    "  <code>create_react_agent</code> has many additional capabilities.  \n",
    "  The course will not be using them so we will just describe them here and save exploring them to another course. \n",
    "\n",
    "  - For reference, the `create_react_agent` call definition is [here](https://langchain-ai.github.io/langgraph/reference/agents/#:~:text=of%20the%20agent.-,create_react_agent,-%C2%B6)\n",
    "\n",
    "  - pre-hook: This inserts a node prior to the agent node. It will have access to `state`. This is often used to manage messages by summarizing or compressing. See more [here](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent-manage-message-history/#keep-the-original-message-history-unmodified:~:text=to%20the%20LLM.-,def%20pre_model_hook(state)%3A,-trimmed_messages%20%3D%20trim_messages))\n",
    "  - post-hook: This inserts a node following the LLM call, prior to the tool call. This is useful for implementing human-in-the-loop, guardrails, validation, or other post-processing\n",
    "  - response_format: This adds a node before `END`. This will call and LLM.with_structured_output and the output will be formatted to match the given schema and returned in the 'structured_response' state key.\n",
    "</td>\n",
    "</tr>\n",
    "</table> \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2dab8-2156-4c1f-87c6-5773f841cbe3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c57dd-a9cc-4e8d-9555-9eb414fb9151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
